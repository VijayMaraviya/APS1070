{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ucvm7IH6da3"
   },
   "source": [
    "# APS1070\n",
    "#### Lab 2 - Anomaly Detection Algorithm using Gaussian Mixture Model \n",
    "**Deadline: Feb 14, 23:59 - 10 points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGTYOwVXnmGv"
   },
   "source": [
    "Please fill out the following:\n",
    "\n",
    "\n",
    "*   Name: \n",
    "*   Student Number: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7eznbYb46UIg"
   },
   "source": [
    "##**Marking Scheme:**\n",
    "\n",
    "This project is worth **10 marks** of your final grade.\n",
    "\n",
    "**One (1) mark** of the lab is dedicated to **vectorized coding**. If you need to write a loop in your solution, think about how you can implement the same functionality with vectorized operations. Try to avoid loops as much as possible (in some cases loops are inevitable).\n",
    "\n",
    "This notebook is composed of two sections, a Tutorial, and an Exercise. \n",
    "\n",
    "The TAs in the lab will help you to complete your tutorial (Although no mark is assigned to the **tutorial** compeleting that section is **mandatory**). \n",
    "\n",
    "**The exercise** section is worth **9 points**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ah6mFIP_8ArM"
   },
   "source": [
    "##**Important Note:**\n",
    "\n",
    "1] Please **write answers** for the Tutorial & Exercise in the **blanks provided**.\n",
    "\n",
    "2] Start lab early so that you understand concepts in Tutorial well & get enough time to implement exercise  \n",
    "\n",
    "3] It is important to **complete and run the tutorial part** of the notebook. Make sure you complete both, tutorial as well as exercise part.\n",
    " \n",
    "4] Provide a graphical representation of data wherever necessary.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hs5-q7uBE5EX"
   },
   "source": [
    "##Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rGO4OT_Ce8Sj"
   },
   "source": [
    "In this part of the assignment, we will implement an anomaly detection algorithm using the Gaussian model to detect anomalous behavior in a 2D dataset first and then a high-dimensional dataset.\n",
    "\n",
    "Loading relevant libraries and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTx2BdY3e8S8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y_true = make_blobs(n_samples=400, centers=1,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "X_append, y_true_append = make_blobs(n_samples=10,centers=1,\n",
    "                                    cluster_std=5,random_state=0)\n",
    "X = np.vstack([X,X_append])\n",
    "y_true = np.hstack([y_true, [1 for _ in y_true_append]])\n",
    "X = X[:, ::-1] # flip axes for better plotting\n",
    "plt.scatter(X[:,0],X[:,1],marker=\"x\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mD8qOcC2e8Te"
   },
   "source": [
    "Here we've manufactured a dataset where some points are visibly outliers from the main distribution.\n",
    "\n",
    "We can see this from looking at the plot, but how do we robustly identify the outliers? \n",
    "\n",
    "That's where a Gaussian estimation comes in. For this dataset, we only need a single Gaussian, for which we are gonna calculate the mean and standard deviation. Then, we're able to find the points that don't seem likely to have originated from that distribution - these are our outliers!\n",
    "\n",
    "First, we need to calculate the mean and variance for our data. Complete the function below to generate these values using these formulas:\n",
    "\n",
    "$$\\mu = \\frac{1}{m} \\sum_{i=1}^{m}X_i$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m}(X_i-\\mu)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MX-SIIr4e8Tk"
   },
   "outputs": [],
   "source": [
    "def estimateGaussian(X):\n",
    "    \"\"\"\n",
    "     This function estimates the parameters of a Gaussian distribution using the data in X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #compute mean of X\n",
    "    sum_ = \n",
    "    mu = \n",
    "    \n",
    "    # compute variance of X\n",
    "    var = \n",
    "    \n",
    "    return mu,var\n",
    "mu, sigma2 = estimateGaussian(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ulJhFFHe8T5"
   },
   "source": [
    "Now, we will calculate for each point in X, the probability of the distribution $N(\\mu,\\sigma^2)$ generating that point randomly. This has been completed for you, although it is important to understand how the calculation of the PDF works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tdvju5Ive8UF"
   },
   "outputs": [],
   "source": [
    "def multivariateGaussian(X, mu, sigma2):\n",
    "    \"\"\"\n",
    "    Computes the probability density function of the multivariate gaussian distribution.\n",
    "    \"\"\"\n",
    "    k = len(mu)\n",
    "    \n",
    "    sigma2=np.diag(sigma2)\n",
    "    X = X - mu.T\n",
    "    p = 1/((2*np.pi)**(k/2)*(np.linalg.det(sigma2)**0.5))* np.exp(-0.5* np.sum(X @ np.linalg.pinv(sigma2) * X,axis=1))\n",
    "    return p\n",
    "p = multivariateGaussian(X, mu, sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_Sze60ie8Uc"
   },
   "source": [
    "Now that we have the probability of each point in the dataset, we can plot these on the original scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POrlps2qe8Uj"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0],X[:,1],marker=\"x\",c=p,cmap='viridis');\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3eLSr5Ae8U4"
   },
   "source": [
    "We're getting closer to the point where we can programmatically  identify our outliers for a single Gaussian distribution. The last step is to identify a value for $p$, below which we consider a point to be an outlier. We term this $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1snRc3pbe8VN"
   },
   "outputs": [],
   "source": [
    "#Choose a value for epsilon\n",
    "\n",
    "epsilon = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbkVvmuBe8Wb"
   },
   "source": [
    "Now we'll highlight on the scatter plot all points that are below $\\epsilon$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jm-ifxxQe8Wn"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,0],X[:,1],marker=\"x\",c=p,cmap='viridis');\n",
    "# Circling of anomalies\n",
    "outliers = np.nonzero(p<epsilon)[0]\n",
    "plt.scatter(X[outliers,0],X[outliers,1],marker=\"o\",facecolor=\"none\",edgecolor=\"r\",s=70);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOxDRW3ce8XA"
   },
   "source": [
    "What value of epsilon did you choose? Why? ____ \n",
    "\n",
    "Play around with different values until you're happy with the plot above. \n",
    "\n",
    "We can additionally look at our model's classification performance another way. By sorting the points ascending by their probability, and classifying the first $k$ points as anomalous, we can determine the model's precision@k. Implement this in the cell below. \n",
    "\n",
    "* What is the highest value of K for which we get a precision of 1.0? ____\n",
    "* What is the precision for K=4? ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_JcEkhk2e8XJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUUfRXXUe8Xg"
   },
   "source": [
    "You may have noticed that in this example, we are training and testing on the _entire_ dataset. This is absolutely not standard practice! You should _always_ split into a training and testing set. However, the reason that we can get away with this here is that we don't actually use labels at all during training - this is an _unsupervised_ machine learning task. Unsupervised learning methods are beneficial for anomaly detection because in the real world (i.e. testing!) we might come across types of outliers that we didn't see during training. We want to use a method that can handle this, and unsupervised methods are often better suited to this type of domain.\n",
    "\n",
    "For the next section, we'll move to a Mixture of Gaussian models. Take a look at the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "un2Y2Ci3e8Xp"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y_true = make_blobs(n_samples=400, centers=5,\n",
    "                       cluster_std=0.60, random_state=1)\n",
    "X_append, y_true_append = make_blobs(n_samples=50,centers=5,\n",
    "                                    cluster_std=5,random_state=1)\n",
    "X = np.vstack([X,X_append])\n",
    "y_true = np.hstack([[0 for _ in y_true], [1 for _ in y_true_append]])\n",
    "X = X[:, ::-1] # flip axes for better plotting\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_true, test_size=0.33, random_state=1, shuffle=True)\n",
    "\n",
    "plt.scatter(X_train[:,0],X_train[:,1],marker=\"x\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fs6_y8BFe8Yb"
   },
   "source": [
    "Okay, we have more than one cluster centre now. So what? Let's just ignore that and use the same model as before. \n",
    "\n",
    "* What is the precision for k=1 in the cell below? ___\n",
    "* k=10? ___\n",
    "* k=100? ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-j_1On8e8Yi"
   },
   "outputs": [],
   "source": [
    "mu, sigma2 = estimateGaussian(X_train)\n",
    "p = multivariateGaussian(X_test, mu, sigma2)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_test[:,0],X_test[:,1],marker=\"x\",c=p,cmap='viridis');\n",
    "outliers = np.nonzero(p<0.001)[0]\n",
    "plt.scatter(X_test[outliers,0],X_test[outliers,1],marker=\"o\",facecolor=\"none\",edgecolor=\"r\",s=70);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kmcR_pCe8Y6"
   },
   "source": [
    "Uh oh. This model performs terribly. It's fit the mean to a section of space where we don't have _any_ points, and it has absolutely no idea which points are outliers! This was probably pretty obvious to you though. We need to move to a Mixture of Gaussians model - one in which we use multiple Gaussians to fit the data. We'll use `sklearn.mixture.GaussianMixture` to do this - or rather you will! Use the documentation, found [here](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) to initialise and fit a `GaussianMixture` object called `gm` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOcXDk-ie8ZD"
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gm = GaussianMixture(n_components = 5,\n",
    "                    covariance_type = 'full', random_state=0, )\n",
    "gm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T1vVv9cHEqkd"
   },
   "source": [
    "Now we can use the method `gm.score_samples()` which gives a score based on how likely a point is to have been generated by any cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZ0g9Suie8Ze"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "c = gm.score_samples(X_train)\n",
    "plt.scatter(X_train[:,0],X_train[:,1],c=gm.score_samples(X_train),cmap='viridis',marker='x')\n",
    "\n",
    "threshold= -4.8\n",
    "\n",
    "outliers = np.nonzero(c<threshold)[0]\n",
    "plt.scatter(X_train[outliers,0],X_train[outliers,1],marker=\"o\",facecolor= \"none\",edgecolor=\"r\",s=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UNHXYGQE051"
   },
   "source": [
    "What is the variable \"`threshold`\"? ______________ Why is it negative? __________________________\n",
    " \n",
    "Now we can use the method `gm.predict_proba()` to spot the points in each of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPOHN1zwE4hN"
   },
   "outputs": [],
   "source": [
    "Non_Outliers=np.nonzero(c>=threshold)[0]\n",
    "X_t=X_train[Non_Outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pupN5PmuE67x"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(5):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    plt.scatter(X_t[:,0],X_t[:,1],c=gm.predict_proba(X_t)[:,i],cmap='viridis',marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4A4pD4jE9kb"
   },
   "source": [
    "1] What do functions `gm.score_samples()` and `gm.predict_proba()` return? ___________ \n",
    "\n",
    "2] Why it was important to run them in above sequence? ____________________\n",
    "\n",
    "3] What is the difference between the two function?_______________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ue-8F_8_e8Zp"
   },
   "source": [
    "Our Mixture of Gaussians model is powerful! Not only is it unsupervised, it can both classify points into one of the K clusters we have, _and_ it can help us with our ultimate goal of identifying outlier points! We can do this by finding the points that no cluster wants to claim for itself.\n",
    "\n",
    "In the cell below, complete the code and calculate these values and then compute precision@k for k=1, 10, and 100. The ROC curve code has been completed for you. \n",
    "\n",
    "* Is this model better or worse performing than the previous? ___\n",
    "* Why might that be? ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNXCnzife8Zt"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "#This part outputs the precision and recall on the test set\n",
    "\n",
    "#score_samples will compute the weighted log probabilities for each sample\n",
    "p_gm = gm.score_samples(X_test) \n",
    "\n",
    "#Complete code below\n",
    "for i in []: \n",
    "   #sort the points by probability, as before\n",
    "   mn_gm = \n",
    "   #compare y_test labels to our picks using precision\n",
    "   precision =  \n",
    "   #compare y_test labels to our picks using recall\n",
    "   recall =  \n",
    "   #print precision and recall three times\n",
    "   print()\n",
    "\n",
    "#This part computes the ROC curves for both models like we talked about in class\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot\n",
    "fpr_sc, tpr_sc, _ = roc_curve(y_test, 1-p)\n",
    "fpr_gm, tpr_gm, _ = roc_curve(y_test, 1-p_gm)\n",
    "pyplot.plot(fpr_sc, tpr_sc, linestyle = '--', label='Single Component')\n",
    "pyplot.plot(fpr_gm, tpr_gm, marker='.', label='Gaussian Mixture')\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ft37989Ze8Z4"
   },
   "source": [
    "Let's look at a dataset that motivates using a Mixture of Gaussians model: Simpsons ratings.\n",
    "\n",
    "Everyone knows that there's a certain point when The Simpsons \"got bad\", but can we use a Mixture of Gaussians to find out exactly when that was?\n",
    "\n",
    "Load up the `simpsons.pickle` file using the cell below. It contains the IMDb rating for every simpsons episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoowFfVve8Z7"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/alexwolson/APS1070_data/raw/master/simpsons.pickle\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "with open('simpsons.pickle','rb') as f:\n",
    "    simpsons = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2vwlvtV9e8ad"
   },
   "source": [
    "Plot a histogram of the rating distribution for all Simpsons episodes. \n",
    "\n",
    "* What is the modal rating? __\n",
    "* What is the range of ratings? __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxZC72Wme8al"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yD3dnH7Ce8at"
   },
   "source": [
    "Next, use Gaussian Mixture to fit a Mixture of Gaussians to the Simpsons rating distribution. Since we are trying to distinguish between good and bad ratings, we only need 2 gaussians.\n",
    "\n",
    "* What are the means for the two Gaussians fit by the model? __\n",
    "* What about the standard deviations? __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HmJ5ESd7e8ay"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GFf8OR1me8a8"
   },
   "source": [
    "Finally, using the `GuassianMixture.predict()` method, we can use maximum likelihood to estimate which distribution, good or bad, each episode belongs to. In the cell below, we have provided code to count the number of episodes predicted to be in the \"good\" distribution per season, and plot for the same.\n",
    "Understand the code and answer the question.\n",
    "\n",
    "* Where is the notable drop-off point? __\n",
    "* What is the first season with 0 good episodes? __\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETyADy8xY0kM"
   },
   "outputs": [],
   "source": [
    "#Let's first associate each component with a good or bad season\n",
    "if gm.means_[0,0] > gm.means_[1,0]: #True if first component is the good season (ie, higher mean)\n",
    "     Good_season_index = 0\n",
    "else:\n",
    "     Good_season_index = 1\n",
    "\n",
    "Xs = []\n",
    "Ys = []\n",
    "simpsons = dict(sorted(list(simpsons.items()), key=lambda x: x[0]))\n",
    "for season, episodes in simpsons.items():\n",
    "     bad = 0\n",
    "     good = 0\n",
    "     for episode in episodes.values():\n",
    "          if gm.predict(np.array(\n",
    "               episode\n",
    "          ).reshape(-1,1)) == Good_season_index:\n",
    "               good += 1\n",
    "          else:\n",
    "               bad += 1\n",
    "     Xs.append(season)\n",
    "     Ys.append(good/(good+bad))\n",
    "plt.plot(Xs,Ys);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rJ4v0HLEyie"
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVh8MGvte8bX"
   },
   "source": [
    "Now we are going to work with a credit card fraud dataset (more detail can be found [here](https://www.kaggle.com/mlg-ulb/creditcardfraud/downloads/creditcardfraud.zip/3)). This dataset contains 28 key features, which are not \n",
    "directly interpretable but contain meaningful information about the dataset.\n",
    "\n",
    "Load up the dataset in CSV file using Pandas. The dataset is called `creditcard.csv`. Print out the first few columns of the dataset.\n",
    "\n",
    "* How many rows are there?[0.5] _____\n",
    "* What features in the dataset are present aside from the 28 main features?[0.5]  _____\n",
    "* Why do you think the main features are given to us in this way?[0.5] _____\n",
    "* Which column contains the targets? Can you figure out what the values correspond to?[0.5]_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9LfYqXUHbql"
   },
   "outputs": [],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sg0gndnDe8bd"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "wget.download('https://github.com/alexwolson/APS1070_data/raw/master/creditcard.tar.gz','creditcard.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4h5z71s8e8bm"
   },
   "outputs": [],
   "source": [
    "!tar -zxvf creditcard.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZAbDaphe8bt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1w6cRXOee8b3"
   },
   "source": [
    "It's always important when looking at a new dataset to figure out how many rows we have for each class.\n",
    "\n",
    "* What is the percentage of entries in the dataset for each class?[0.5] _____\n",
    "* Is this target data balanced or unbalanced? why do you think this is so?[0.5]_____\n",
    "* Why might this pose a problem when methods we have looked at so far?[1] _____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v4EPw3I-e8b7"
   },
   "source": [
    "Next, split the dataset into a training and testing set. Use a test size of one third, and set the random state to 0.\n",
    "\n",
    "Make sure to separate out the column corresponding to the targets.\n",
    "\n",
    "As mentioned earlier, in this lab we are going to use Gaussian distributions to model the data. To accomplish this, we are going to introduce `scipy`, a package which contains a wide variety of tools for working with scientific data in Python. Its `stats` package allows us to easily model various statistical distributions, and get information about them.\n",
    "\n",
    "Scipy's Gaussian distribution class is called `norm`. It takes two parameters - `loc`, which corresponds to the mean of your distribution, and `scale`, which corresponds to the standard deviation.\n",
    "\n",
    "* What are the mean and standard deviation for variable V24? Make sure to only use your training set for this. [1] _____\n",
    "\n",
    "Use the code below to set up a Gaussian object for V24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbvAZ42qe8b9"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "norm = stats.norm(\n",
    "    #Put your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSFk1vjne8cG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "x = np.linspace(norm.ppf(0.01),\n",
    "                norm.ppf(0.99), 100)\n",
    "ax.plot(x, norm.pdf(x),\n",
    "       'r-', lw=5, alpha=0.6, label='norm pdf')\n",
    "ax.hist(X_train['V24'].values, density=True, histtype='stepfilled', bins=50);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qe41hNLJe8cQ"
   },
   "source": [
    "We can also look at the difference in distribution for some variables between fraudulent and non-fraudulent transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTGw4xNde8cV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "features=[f for f in df.columns if 'V' in f]\n",
    "nplots=np.size(features)\n",
    "plt.figure(figsize=(15,4*nplots))\n",
    "gs = gridspec.GridSpec(nplots,1)\n",
    "for i, feat in enumerate(features):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    sns.distplot(X_train[feat][y_train==1], bins=30)\n",
    "    sns.distplot(X_train[feat][y_train==0],bins=30)\n",
    "    ax.legend(['fraudulent', 'non-fraudulent'],loc='best')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title('Distribution of feature: ' + feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ac1nyvCPe8ce"
   },
   "source": [
    "1] Build an outlier detection model using a Mixture of Gaussians using five features with a large difference in distribution between fraudulent and non-fraudulent data. Report the precision and recall @k for k=1, 10 and 100 (on test data). Repeat this model creation process, this time selecting five features with a of your own choice, and reporting the precision and recall as above. [2]\n",
    "\n",
    "2] What features did you choose for each model? What effect it had on the model performance? [1] ___________\n",
    "\n",
    "3] Organize your findings in a table or plot and describe your findings. What can you conclude about what models work best?[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yClYMXloe8cg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project2_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
